<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Metric-Free Exploration for Topological Mapping by Task and Motion Imitation in Feature Space">
  <meta name="keywords" content="VPR, Self-Supervised, SLAM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DeepExplorer</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script> -->
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <link rel="shortcut icon" type="image/x-icon" href="./static/images/ai4ce.png" />
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://ai4ce.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            <!-- <img src="./static/images/logo.png" width="45"> -->
            DeepExplorer&nbsp;&nbsp
          </h1>
          <h1 class="title is-2 publication-title">Metric-Free Exploration for Topological Mapping by Task and Motion Imitation in Feature Space</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Yuhang _____*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://zichuanfang2015@yahoo.com">Irving Fang*</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=i_aajNoAAAAJ">Yiming Li</a><sup>2</sup>
            </span>
            <span class="author-block">
              <a href="https://rushibs.github.io">Rushi Bhavesh Shah</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=YeG8ZM0AAAAJ">Chen Feng</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup> Yuhang's University</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>2</sup>New York University, Brooklyn, NY 11201, USA</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                 <!-- add here later. -->
                <a href="arxiv.org university"                
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <!-- add here later. -->
               <!-- <a href=""                    
                  class="external-link button is-normal is-rounded is-dark">
                 <span class="icon">
                     <i class="fas fa-camera"></i>
                 </span>
                 <span>Appendix</span>
               </a> -->
             </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/ai4ce/DeepExplorer"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <!-- add here later -->
                <a href="    "
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video id="teaser" controls loop height="100%">
        <source src="./static/video/Multimedia.mp4"
                type="video/mp4">
      </video> -->
      <!-- <img class="rounded" src="./media/nice-slam/teaser.png" > -->
      <!-- <br><br><br> -->
      <!-- <h2 class="subtitle has-text-centered">
      Project explanation
      </h2> -->
    <!-- <h2 class="subtitle has-text-centered">
      (The <span style="color:#000000;">black</span> / <span style="color:#ff0000;">red</span> lines are the ground truth / predicted camera trajectory)
    </h2> -->    
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video id="teaser" autoplay muted loop height="100%">
        <source src="./static/images/SUBT_video.mp4"
                type="video/mp4">
      </video> -->
      <!-- <img src="./static/images/kitti_map.jpg" height="300"> -->
      <img class="rounded" src="./static/images/MP3D_allvisexp.png" >
      <br><br><br>
      <h2 class="subtitle has-text-centered">
          DeepExplorer exploration result on <b>MP3D room scene q9vSo1VnCiC</b>. We show both efficient exploration results in top row, sub-figure A, B, C, and
          also relatively less-efficient exploration in sub-figure E and F, which are mainly due to local repetitive exploration. We further show an inefficient
          exploration example in sub-figure D. Two room scene 3D visualization is given in the left-most subfigures. The agent exploration starting-position is marked
          by a red rectangle patch.</h2>
    <!-- <h2 class="subtitle has-text-centered">
      (The <span style="color:#000000;">black</span> / <span style="color:#ff0000;">red</span> lines are the ground truth / predicted camera trajectory)
    </h2> -->
    <!-- <h2 class="is-size-6 has-text-centered">(The left are few-shot annotations online provided by user and right is the detection results)</h2> -->
    
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
        <!-- Paper video. -->
        <!-- <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Video</h2>
            <div class="publication-video">
              <iframe src="https://www.youtube.com/embed/V5hYTz5os0M?rel=0&amp;showinfo=0"
                      frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </div>
          </div>
        </div> -->
        <!--/ Paper video. -->
      
    <!-- <br> -->
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
         <p>
          We propose DeepExplorer, a simple and lightweight metric-free exploration method for topological mapping of unknown environments. 
          It performs <b>Task And Motion Planning (TAMP)</b> entirely in image feature space.</p>
          <p>
            <li>The <b>Task Planner</b> is a recurrent network using the latest image observation sequence to hallucinate a feature as the next-best exploration goal. </li>
            <li>The <b>Motion Planner</b> then utilizes the current and the hallucinated features to generate an action taking the agent towards that goal. </li>
          </p>  
          <p>
          The two planners are jointly trained via deeply-supervised imitation learning from expert demonstrations. 
          During exploration, we iteratively call the two planners to predict the next action, and the topological map is built by 
          constantly appending the latest image observation and action to the map and using visual place recognition (VPR) for loop closing.
          </p>
          <p>
            The resulting topological map efficiently represents an environment's connectivity and traversability, so it can be used for tasks such as visual navigation. 
            We show DeepExplorer's exploration efficiency and strong sim2sim generalization capability on large-scale simulation datasets like Gibson and MP3D. 
            Its effectiveness is further validated via the image-goal navigation performance on the resulting topological map. We further show its strong zero-shot sim2real generalization capability in real-world experiments.
          </p>
          <!DOCTYPE html>
          <html>
          <head>
            <meta charset="utf-8">
            <meta name="description"
                  content="Self supervised VPR method to detect spatial Neighborhoods">
            <meta name="keywords" content="VPR, Self-Supervised, SLAM">
            <meta name="viewport" content="width=device-width, initial-scale=1">
            <title>DeepExplorer</title>
          
            <!-- Global site tag (gtag.js) - Google Analytics -->
            <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
            <script>
              window.dataLayer = window.dataLayer || [];
          
              function gtag() {
                dataLayer.push(arguments);
              }
          
              gtag('js', new Date());
          
              gtag('config', 'G-PYVRSFMDRL');
            </script> -->
            <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
                  rel="stylesheet">
          
            <link rel="stylesheet" href="./static/css/bulma.min.css">
            <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
            <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
            <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
            <link rel="stylesheet"
                  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
            <link rel="stylesheet" href="./static/css/index.css">
          
            <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script> -->
            <script defer src="./static/js/fontawesome.all.min.js"></script>
            <script src="./static/js/bulma-carousel.min.js"></script>
            <script src="./static/js/bulma-slider.min.js"></script>
            <script src="./static/js/index.js"></script>
            <link rel="shortcut icon" type="image/x-icon" href="./static/images/ai4ce.png" />
          </head>
          <body>        
            </div>
          </nav>
          
          <section class="section">
            <div class="container is-max-desktop">
              <!-- Method. -->
              <div class="columns is-centered has-text-centered">
                <div class="column is-full_width">
                  <hr>
                  <h2 class="title is-3">Method</h2>
                  <br>
                  <img src="./static/images/Topo_map_ver8.png" class="center"/>
                  <div class="content has-text-justified">
                    <br>  
                    <p>
                    SOMETHING HERE
                      <li> 
                          SOMETHING HERE</li>
                      <li> 
                          SOMETHING HERE</li>
                    </p>
                  </div>
                </div>
              </div>
              <hr>
          
              
              <!-- Applications.-->
              <div class="columns is-centered has-text-centered">
                <div class="column is-full_width">
                  <h2 class="title is-3">Dataset and Exploration results</h2>
                </div>
              </div>
              <p>
                &nbsp
              </p>
              <div class="column is-full_width">
                <!-- <img src="./static/images/kitti_map.jpg" width=1080 border=2px class="center"/> &nbsp;&nbsp; -->
                <!-- <img src="./static/images/nclt_map.jpg"  width=1080 border=2px class="center"/> &nbsp;&nbsp; -->
                <!-- <img src="./static/images/nebula_map.jpg" width=1080 border=2px class="center"/> -->
                <video muted autoplay loop controls>
                  <source src="./static/videos/exploredemo_Gibson_step0.25_turnangle10/exploretraj_Gibson_mosquito_1.mp4" type="video/mp4">
                </video>
                <br> <br>
                <video muted autoplay loop controls>
                  <source src="./static/videos/exploredemo_MP3D_step0.25_turnangle10/explore_floor0_q9vSo1VnCiC_3.mp4" type=video/mp4>  
                </video>
                <br> <br>
                <video  muted autoplay loop controls>
                  <source src="./static/videos/exploredemo_RealWorld_step0.25_turnangle10/zeroshot_success.mp4" type=video/mp4>  
                </video>
                
                <!-- <img src="./static/videos/exploredemo_RealWorld_step0.25_turnangle10/zeroshot_successexplore_trajectory.png" width= height="" border=2px>  -->
                <br> <br>
                <div class="content has-text-justified">
                  <br>  
                  <p>
                    (a) <a href="http://gibsonenv.stanford.edu/">Gibson dataset</a>
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (b) <a href="  ">MP3D dataset</a>
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (c) <a href="  ">Real-World indoor dataset </a>
                    <br>
                    <br>
                    We employ three datasets for a comprehensive evaluation: (1) Gibson dataset, (2) MatterPort3D (MP3D) dataset, and (3) Real-World indoor dataset.
                  </p>
          
                  <p>
                    <em><b>Gibson dataset :</b></em> SOMETHING HERE</p>
                  <p>
                    <em><b>MatterPort3D (MP3D) dataset : </b></em> SOMETHING HERE</p>
                  <p>
                    <em><b>Real-World indoor dataset : </b></em>SOMETHING HERE</p>
                </div>
              </div>
              <p>
                &nbsp
              </p>
              <p>
                &nbsp
              </p>
          
          
              <section class="section">
                <div class="container is-max-desktop">
                  <!-- <br> -->
                  <!-- Abstract. -->
                  <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                      <h2 class="title is-3">Training animation</h2>
                      <div class="column is-full_width">
                        <img src="static\images\KITTI_0018.gif" width=1000 height=90% overflow=hidden class="center"/>
                        <h3 class="title is-4">
                            We demostration one <em><b>KITTI</b></em> mapping result, translation ATE and rotation ATE with respect to the epochs. Each pose is represented by an arrow indicating the xy-coordinate and heading (yaw angle) as shown in the bottom examples. The color indicates the frame index in the trajectory.
                        </h3>
                        <p>
                          &nbsp
                        </p>
                        <img src="static\images\KITTI_0027.gif" width=1000 height=100px overflow=hidden class="center"/>
                        <h3 class="title is-4">
                            We demostration another <em><b>KITTI</b></em> mapping result, translation ATE and rotation ATE with respect to the epochs. The representation is the same as above.
                        </h3>
                        <p>
                          &nbsp
                        </p>
                        <img src="static\images\NCLT_0108.gif" width=1000 height=100px overflow=hidden class="center"/>
                        <h3 class="title is-4">
                            We demostration one <em><b>NCLT</b></em> mapping result, translation ATE and rotation ATE with respect to the epochs. The representation is the same as above.
                        </h3>
                        <p>
                          &nbsp
                        </p>
                        <img src="static\images\Nebula_E.gif" width=1000 height=100px overflow=hidden class="center"/>
                        <h3 class="title is-4">
                            We demostration another <em><b>Nebula</b></em> mapping result with respect to the epochs. The ground truth in this dataset is not accurate, thus translation ATE and rotation ATE are not provided. The representation is the same as above.
                        </h3>
                        <div class="content has-text-justified">
                   
                        </div>
                      </div>
                    </div>
                  </div>
                  <!--/ Abstract. -->
                </div>
              </section>
          
          
              
          
          
          <section class="section" id="BibTeX">
            <div class="container is-max-desktop content">
              <h2 class="title">BibTeX</h2>
              <pre><code>
                @article{chen2022deepmapping2,
                  title={DeepExplorer: Self-Supervised Large-Scale LiDAR Map Optimization},
                  author={Chen, Chao and Liu, Xinhao and Li, Yiming and Ding, Li and Feng, Chen},
                  journal={arXiv preprint arXiv:2212.06331},
                  year={2022}
                }
              </code></pre>
            </div>
          </section>
          
          <section class="section" id="Acknowledgements">
            <div class="container is-max-desktop content">
              <h2 class="title">Acknowledgements</h2>
              Chen Feng is the corresponding author. The research is supported by NSF Future Manufacturing program under CMMI-1932187, CNS-2121391, and EEC-2036870. Chao Chen gratefully thanks the help from Pratyaksh P. Rao and Pareese Pathak.
            </div>
          </section>
          
          
          <footer class="footer">
            <div class="container">
              <div class="content has-text-centered">
              </div>
              <div class="columns is-centered">
                <div class="column is-8">
                  <div class="content">
                    <p>
                      This website is licensed under a <a rel="license"
                                                          href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                      Commons Attribution-ShareAlike 4.0 International License</a>.
                      This webpage template is from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. 
                      We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
                    </p>
                  </div>
                </div>
                    </p>
                  </div>
                </div>
              </div>
            </div>
          </footer>
          
          </body>
          </html>
          
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full_width">
        <hr>
        <h2 class="title is-3">Method</h2>
        <br>
        <img src="./static/images/pipeline2.png" class="center"/>
        <div class="content has-text-justified">
          <br>  
          <p>
          Our contributions are summarized as follows:
            <li> 
                Our DeepExplorer is the first self-supervised large-scale LiDAR map optimization method as far as we know, and this generic method achieves state-of-the-art mapping results on various indoor/outdoor public datasets, including KITTI, NCLT, and the challenging underground dataset Nebula. Our code will be released to ensure reproducibility.
            </li>
            <li> 
                Our analysis reveals why DeepMapping fails to scale up and leads to the two novel techniques to incorporate loop closing and local registration in the DeepMapping framework. The necessity and effectiveness of the two techniques are further validated in our ablation study.
            </li>
          </p>
        </div>
      </div>
    </div>
    <hr>

    
    <!-- Applications.-->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full_width">
        <h2 class="title is-3">Dataset and Mapping results</h2>
      </div>
    </div>
    <p>
      &nbsp
    </p>
    <div class="column is-full_width">
      <img src="./static/images/kitti_map.jpg" width=1080 border=2px class="center"/> &nbsp;&nbsp;
      <img src="./static/images/nclt_map.jpg"  width=1080 border=2px class="center"/> &nbsp;&nbsp;
      <img src="./static/images/nebula_map.jpg" width=1080 border=2px class="center"/>
      <div class="content has-text-justified">
        <br>  
        <p>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(a) <a href="https://www.cvlibs.net/datasets/kitti/">KITTI Data</a>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (b) <a href="http://robots.engin.umich.edu/nclt/">NCLT Data</a>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (c) <a href="https://github.com/NeBula-Autonomy/nebula-odometry-dataset">Nebula Data</a>
          <br>
          <br>
          We employ three datasets for a comprehensive evaluation: (1) KITTI dataset for evaluations in outdoor scenarios, (2) NeBula odometry dataset for evaluations in indoor environments where GPS signal is not available, and (3) campus-level NCLT dataset for evaluations in both indoor and outdoor scenarios.
        </p>

        <p>
          <em><b>KITTI dataset</b></em> is a widely-used authoritative benchmark to evaluate SLAM-based algorithms. We employ the two most complex and challenging scenes from the dataset, where lots of places are revisited multiple times and the explored area is relatively large. Meanwhile, there are dynamic objects on the road, further increasing the difficulties.
        </p>
        <p>
          <em><b>NCLT dataset</b></em> is a large-scale LiDAR dataset collected in the University of Michiganâ€™s North Campus. The point clouds are collected using a Velodyne HDL-32E 3D LiDAR mounted on a Segway robot. Overall, the NCLT dataset has a robot trajectory with a length of 147.4 km and maintains 27 discrete mapping sessions over the year. Each mapping session includes both indoor and outdoor environments. We select an interval of the trajectory for better illustration.
        </p>
        <p>
          <em><b>Nebula dataset:</b></em> is provided by the Team CoSTAR from the Jet Propulsion Laboratory at Caltech. The dataset has been used to test the multi-robot system in real-world environments such as tunnels and caves for the DARPA Subterranean Challenge. In each scene of the dataset, a ground-truth survey-grade map is provided by DARPA and the trajectory is produced by running LOCUS 2.0 on the map. Besides the point clouds, visual odometry and kinematic odometry are also included. We employ the data collected in the cave in Lava Beds National Monument. The whole trajectory is 590.85 meters long and contains 37,949 LiDAR scans in total.
        </p>
      </div>
    </div>
    <p>
      &nbsp
    </p>
    <p>
      &nbsp
    </p>


    <section class="section">
      <div class="container is-max-desktop">
        <!-- <br> -->
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Training animation</h2>
            <div class="column is-full_width">
              <img src="static\images\KITTI_0018.gif" width=1000 height=90% overflow=hidden class="center"/>
              <h3 class="title is-4">
                  We demostration one <em><b>KITTI</b></em> mapping result, translation ATE and rotation ATE with respect to the epochs. Each pose is represented by an arrow indicating the xy-coordinate and heading (yaw angle) as shown in the bottom examples. The color indicates the frame index in the trajectory.
              </h3>
              <p>
                &nbsp
              </p>
              <img src="static\images\KITTI_0027.gif" width=1000 height=100px overflow=hidden class="center"/>
              <h3 class="title is-4">
                  We demostration another <em><b>KITTI</b></em> mapping result, translation ATE and rotation ATE with respect to the epochs. The representation is the same as above.
              </h3>
              <p>
                &nbsp
              </p>
              <img src="static\images\NCLT_0108.gif" width=1000 height=100px overflow=hidden class="center"/>
              <h3 class="title is-4">
                  We demostration one <em><b>NCLT</b></em> mapping result, translation ATE and rotation ATE with respect to the epochs. The representation is the same as above.
              </h3>
              <p>
                &nbsp
              </p>
              <img src="static\images\Nebula_E.gif" width=1000 height=100px overflow=hidden class="center"/>
              <h3 class="title is-4">
                  We demostration another <em><b>Nebula</b></em> mapping result with respect to the epochs. The ground truth in this dataset is not accurate, thus translation ATE and rotation ATE are not provided. The representation is the same as above.
              </h3>
              <div class="content has-text-justified">
         
              </div>
            </div>
          </div>
        </div>
        <!--/ Abstract. -->
      </div>
    </section>


    


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @article{chen2022deepmapping2,
        title={DeepExplorer: Self-Supervised Large-Scale LiDAR Map Optimization},
        author={Chen, Chao and Liu, Xinhao and Li, Yiming and Ding, Li and Feng, Chen},
        journal={arXiv preprint arXiv:2212.06331},
        year={2022}
      }
    </code></pre>
  </div>
</section>

<section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
    Chen Feng is the corresponding author. The research is supported by NSF Future Manufacturing program under CMMI-1932187, CNS-2121391, and EEC-2036870. Chao Chen gratefully thanks the help from Pratyaksh P. Rao and Pareese Pathak.
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            This webpage template is from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. 
            We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
          </p>
        </div>
      </div>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
